---
title: "Chpter 6 - Exercies"
author: "Corrie"
date: "July 8, 2018"
output: 
  github_document:
    pandoc_args: --webtex 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

# Chapter 6 - Exercises
These are my solutions to the exercises from chapter 6.

## Easy. 
__6E1.__ State the three motivating criteria that define information entropy.

Information entropy (a measure of uncertainty) should be

- _continous_. A small change in probability should also lead to only a small change in uncertainty. We don't want to allow for sudden jumps.
- _increasing_ as the number of possible events increases. That means, if only one event has a very high chance of happening and all other have only a very small chance, then there is little uncertainty in what comes next and thus more less information. On the other hand, if all events are equally likely, there is the most uncertainty about what comes next and thus most information.
- _additive_. If we take the uncertainty of two independent distributions, e.g. rain or not (two events) and if I drink coffee in the morning or not (also two events), these events are independent of each other. Then taking the uncertainty for the distribution over all four combinations (rain and coffee, rain and no coffee, etc) is the same as taking the sum over the single distributions. Note that if the two distributions are dependent of each other (maybe instead of me drinking coffee, take the event me biking to work or taking the bus), then the uncertainty is not additive.

__6E2.__ Suppose a coin is weighted such that it comes up head 70% of the time. What is the entropy of this coin?
```{r}
p <- c(0.7, 0.3)
entropy <- function(p) - sum( p[p!=0] * log(p[p!=0]) ) # to avoid NaN errors when probability = 0
entropy(p)
```

__6E3.__ Suppose a four-sided die is loaded such that it shows up with the following probabilities: 1 comes up 20%, 2 comes up 25%, 3 comes up 25% and 4 comes up 30%. What is the entropy of this die?
```{r}
p_die <- c(0.2, 0.25, 0.25, 0.3)
entropy(p_die)
```

__6E4.__ Suppose another four-sided die is loaded such that it never shows 4. The other three sides show equally often. What is the entropy of this die?
```{r}
p_die2 <- c(1/3, 1/3, 1/3, 0) 
entropy(p_die2)
```
This is the same entropy as for the three-sided fair die.

## Medium.
__6M1.__ Compare the definitions of AIC, DIC, and WAIC. Which of these criteria is most general, which assumptions are required to transform a more general criterion into a less general one?
The criteria from the least to the most general:

__AIC__: 

$\text{AIC} = D_{train} + 2p$ where $D_{train}$ is the deviance of the training set and $p$ is the number of parameters. AIC provides an estimate for the test deviance when the following assumptions are met:

  - Priors are flat or overwhelmed by the likelihood.
  - The posterior distribution is approximately multivariate Gaussian.
  - The sample size $N$ is much greater than the number of parameters.
  
__DIC__: 

$\text{DIC} =  \hat{D} + 2p_D$ where $2p_D = \bar{D} - \hat{D}$. Furthermore, $\hat{D}$ is the deviance computed using the mean of the posterior sample and $\bar{D}$ is the mean of the posterior deviance. The DIC is only valid when the following assumptions are met:

  - The posterior distribution is approximately multivariate Gaussian.
  - The sample size $N$ is much greater than the number of parameters.
  
__WAIC__: 

$\text{WAIC} = -2(\text{lppd} - p_{WAIC})$ where $\text{lppd} = \sum_{i=1}^N \log \text{Pr}(y_i)$ is the log-pointwise-predictive-density, i.e. over all observations the sum of the average likelihood of each observation. $p_{WAIC} = \sum_{i=1}^N V(y_i)$ is the effective number of parameters with $V(y_i)$ being the variance in log-likelihood for observation observation $i$. The WAIC is valid when the following assumption is met:

  - The sample size $N$ is much greater than the number of parameters.
  
__6M2.__ Explain the difference between model _selection_ and model _averaging_. What information is lost under model selection? What information is lost under model averaging?

- _Model selection_: In model selection, we use the information criteria to select one model: Among multiple models (all fit to the same observations!), we pick the one that has the best information criteria value. Best means for AIC, DIC, and WAIC the lowest value. This procedure discards information about relative model accuracy contained in the differences among the AIC, DIC, and WAIC values. Model comparison instead uses the DIC or WAIC together with other information from the models (estimate, posterior predictives etc) to find out why one model performs better than another. Information criteria can, for example, help to detect masking associations.
- _Model averaging_: In model averaging, the DIC or WAIC are used to construct a posterior predictive distribution that combines all models. The DIC or WAIC are used to compute weights for each model, and these weights are then used to average the posterior predictive distributions of the models to one single distribution. Model averaging is a rather conservative procedure, meaning that it will never make a predictor variable appear more influential than it already appears in any single model.

__6M3.__ When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments.

```{r, message=FALSE}
library(rethinking)
# generate some data
set.seed(1234)
x <-  rnorm(200, mean=1, sd=1.5)
x1 <- rnorm(200, mean=0, sd=1)            # not associated with outcome
y <- rnorm(200, mean = 5 + 2.4*x, sd=1)
d <- data.frame(x=x, y=y, x1=x1)
```
_Control:_: Three models are fit on the same data, two using the same linear model, the last one using a third predictor variable (that is not associated with the outcome).
```{r}
mod1 <- lm( y ~ x, data=d)
mod2 <- lm( y ~ x, data=d)
mod3 <- lm( y ~ x + x1, data=d)
control <- compare( mod1, mod2, mod3 )
control
```
The first two models have essentially the same WAIC and have approximately equal weights. The last model has a slightly higher WAIC and lower weight.

_First experiment_: Two models are fit with the same linear model, but using different data (but with the same number of observations).
```{r}
mod1_e1 <- lm( y ~ x, data=d, subset=1:33)
mod2_e1 <- lm( y ~ x, data=d, subset=34:66)
mod3_e1 <- lm( y ~ x, data=d, subset=67:99)
exp1 <- compare( mod1_e1, mod2_e1, mod3_e1)
exp1
```
The WAIC differ quite a bit even though all models are fit with the same linear model. Almost weight is on the first model, and the other two have little or zero weight.

_Second experiment_:
```{r}
mod1_e2 <- lm( y ~ x + x1, data=d, subset=1:33)
mod2_e2 <- lm( y ~ x, data=d, subset=34:66)
mod3_e2 <- lm( y ~ x + x1, data=d, subset=67:99)
exp2 <- compare( mod1_e2, mod2_e2, mod3_e2)
exp2
```
Now the first has a much lower WAIC and basically all weight is on the first model. The first and the last model both use the unnecessary (since not associated) variable `x1` but this does not become apparent in the WAICs.

__6M4.__ What happens to the effective number of parameters, as measured by DIC or WAIC, as a prior becomes more concentrated? Why? Perform some experiments.

We use the same data again as before.
```{r}
x <-  rnorm(100, mean=1, sd=2)
x1 <- rnorm(100, mean=0, sd=1)            # not associated with outcome
x2 <- rnorm(100, mean=x, sd=2)            # spurious assocation
y <- rnorm(100, mean = 2 + 2.4*x, sd=2)
d <- data.frame( y=y, x=x, x1=x1, x2=x2)
pairs(d)
```

We run 6 models using the data as above, each time with a different prior. The models thus have 5 parameters: `a, b, b1, b2` and `sigma` of which only `a, b` and `sigma` are actually relevant. 
```{r, results="hide"}
N <- 10    # Number of experiments
dic.l <- list()
waic.l <- list()
for (i in 1:N){
  # generate new data
  x <-  rnorm(100, mean=1, sd=2)
  x1 <- rnorm(100, mean=0, sd=1)            # not associated with outcome
  x2 <- rnorm(100, mean=x, sd=2)            # spurious assocation
  y <- rnorm(100, mean = 2 + 2.4*x, sd=2)
  d <- data.frame( y=y, x=x, x1=x1, x2=x2)
  
  # run the same model with 6 different priors
  mod_100 <- map(
    alist(
      y ~ dnorm( mu, sigma) ,
      mu <- a + b*x +b1*x1 + b2*x2,
      c(b,b1,b2) ~ dnorm(0, 100),
      a ~ dnorm(0, 100),
      sigma ~ dunif(0, 100)
    ), data=d, start=list(b=0, b1=0, b2=0, a=5, sigma=10)
  )
  dic.l[["100"]] <- c(dic.l[["100"]], attr(DIC(mod_100), "pD"))
  waic.l[["100"]] <- c(waic.l[["100"]], attr(WAIC(mod_100), "pWAIC"))
  
  mod_10 <- map(
    alist(
      y ~ dnorm(mu, sigma),
      mu <- a + b*x + b1*x1 +b2*x2,
      c(b, b1, b2) ~ dnorm(0, 10 ),
      a ~ dnorm(0, 10),
      sigma ~ dunif(0, 50)
    ), data=d, start=list(b=0, b1=0, b2=0, a=5, sigma=10)
  )
  
  dic.l[["10"]] <- c(dic.l[["10"]], attr(DIC(mod_10), "pD"))
  waic.l[["10"]] <- c(waic.l[["10"]], attr(WAIC(mod_10), "pWAIC"))
  
  mod_1 <- map(
    alist(
      y ~ dnorm(mu, sigma),
      mu <- a + b*x + b1*x1 + b2*x2,
      c(b, b1,b2) ~ dnorm(0, 1 ),
      a ~ dnorm(0, 1),
      sigma ~ dunif(0, 20)
    ), data=d, start=list(b=0, b1=0, b2=0, a=5, sigma=10)
  )
  
  dic.l[["1"]] <- c(dic.l[["1"]], attr(DIC(mod_1), "pD"))
  waic.l[["1"]] <- c(waic.l[["1"]], attr(WAIC(mod_1), "pWAIC"))
  
  mod_0.5 <- map(
    alist(
      y ~ dnorm(mu, sigma),
      mu <- a + b*x + b1*x1 +b2*x2,
      c(b, b1,b2) ~ dnorm(0, 0.5 ),
      a ~ dnorm(0, 0.5),
      sigma ~ dunif(0, 15)
    ), data=d, start=list(b=0, b1=0, b2=0, a=5, sigma=10)
  )
  
  dic.l[["0.5"]] <- c(dic.l[["0.5"]], attr(DIC(mod_0.5), "pD"))
  waic.l[["0.5"]] <- c(waic.l[["0.5"]], attr(WAIC(mod_0.5), "pWAIC"))
  
  mod_0.2 <- map(
    alist(
      y ~ dnorm(mu, sigma),
      mu <- a + b*x + b1*x1 +b2*x2,
      c(b, b1,b2) ~ dnorm(0, 0.2 ),
      a ~ dnorm(0, 0.2),
      sigma ~ dunif(0, 10)
    ), data=d, start=list(b=0, b1=0, b2=0, a=5, sigma=5)
  )
  
  dic.l[["0.2"]] <- c(dic.l[["0.2"]], attr(DIC(mod_0.2), "pD"))
  waic.l[["0.2"]] <- c(waic.l[["0.2"]], attr(WAIC(mod_0.2), "pWAIC"))
  
  
  mod_0.1 <- map(
    alist(
      y ~ dnorm(mu, sigma),
      mu <- a + b*x + b1*x1 +b2*x2,
      c(b, b1,b2) ~ dnorm(0, 0.1 ),
      a ~ dnorm(0, 0.1),
      sigma ~ dunif(0, 10)
    ), data=d, start=list(b=0, b1=0, b2=0, a=5, sigma=5)
  )
  
  dic.l[["0.1"]] <- c(dic.l[["0.1"]], attr(DIC(mod_0.1), "pD"))
  waic.l[["0.1"]] <- c(waic.l[["0.1"]], attr(WAIC(mod_0.1), "pWAIC"))
  
  mod_0.01 <- map(
    alist(
      y ~ dnorm(mu, sigma),
      mu <- a + b*x + b1*x1 +b2*x2,
      c(b, b1,b2) ~ dnorm(0, 0.01 ),
      a ~ dnorm(0, 10),
      sigma ~ dunif(0, 10)
    ), data=d, start=list(b=0, b1=0, b2=0, a=5, sigma=5)
  )
  
  dic.l[["0.01"]] <- c(dic.l[["0.01"]], attr(DIC(mod_0.01), "pD"))
  waic.l[["0.01"]] <- c(waic.l[["0.01"]], attr(WAIC(mod_0.01), "pWAIC"))
  
}
```

```{r}
options(scipen = 3)
prior <- c(100, 10, 1, 0.5, 0.2, 0.1, 0.01)
prior2 <- c(110, 11, 1.1, 0.55, 0.22, 0.15, 0.015)
dic.mean <- sapply(dic.l, mean)
dic.sd <- sapply(dic.l, sd)
waic.mean <- sapply(waic.l, mean)
waic.sd <- sapply(waic.l, sd)
plot(prior , dic.mean, ylim=c(0,6),
       xlab="Prior", ylab="Number of parameters", log="x",
      pch=16, cex=1, col="steelblue", type="o",
     main="Number of parameters")
points(prior2, waic.mean, cex=1, type="o")

for ( i in 1:length(prior)) {
  pts_in <- dic.mean[i] + c(-1,1)*dic.sd[i] 
  pts_out <- waic.mean[i] + c(-1,1)*waic.sd[i]
  lines( c(prior[i],prior[i]), pts_in, col="steelblue", lwd=2)
  lines( c(prior2[i],prior2[i]), pts_out, lwd=2 )
}
legend("bottomright", c("DIC", "WAIC"), pch=c(16, 1), lwd=c(1,1), 
       col=c("steelblue", "black"), bty="n")
```


The more concentrated a prior becomes, the smaller DIC and WAIC become. They're both bounded from above by the number of used parameters (here 5) but can go lower than the actual relevant parameters (here 3).

__6M5.__ Provide an informal explanation of why informative priors reduce overfitting.
Overfitting occurs when there are too many parameters (for too little data). If these parameters are allowed to vary, they can fit the data too closely, such that the curve perfectly matches the data, including all the noise and it generalizes poorly to new data. If on the other hand, we use informative priors, we restrict the parameters to only vary in the bounds of the prior. It thus limits strongly the degree to which the model can overfit to the data.


__6M6.__ Provide an informal explanation of why overly informative priors result in underfitting.
If we put very restrictive priors on a parameter, the parameter is not allowed to vary much and has to stay inside the bounds of the prior. If then the true parameter is outside these bounds, the model can not learn this parameter, or only with a lot of data.
Compare for example the following two models, one with a very restrictive prior and one with a weakly informative prior on the parameter coefficients.
```{r}
mod_0.01 <- map(
    alist(
      y ~ dnorm(mu, sigma),
      mu <- a + b*x + b1*x1 +b2*x2,
      c(b, b1,b2) ~ dnorm(0, 0.01 ),
      a ~ dnorm(0, 0.01),
      sigma ~ dunif(0, 10)
    ), data=d
  )

  mod_1 <- map(
    alist(
      y ~ dnorm(mu, sigma),
      mu <- a + b*x + b1*x1 + b2*x2,
      c(b, b1,b2) ~ dnorm(0, 1 ),
      a ~ dnorm(0, 1),
      sigma ~ dunif(0, 10)
    ), data=d
  )
  
coeftab_plot( coeftab( mod_0.01, mod_1) )
```

Both models correctly identify that `b1` and `b2` are not relevant to predict the data, but the restrictive model severely underfits and sets all parameters (except `sigma`) to 0.

## Hard.
All practice problems to follow use the !Kung demography data.
We split the data in two equally sized subsets.
```{r}
data("Howell1")
d <- Howell1
d$age <- (d$age - mean(d$age) ) / sd(d$age)
set.seed(1000)
i <- sample( 1:nrow(d), size=nrow(d)/2)
d1 <- d[ i, ]
d2 <- d[ -i,]
```
We will use `d1` to fit models and evaluate them on `d2`.
We will predict the height $h_i$ using the centered age values $x_i$ with the following models:

$$\begin{align*}
\mathcal{M}_1 : \,& h_i \sim \text{Normal}(\mu_i, \sigma) \\
& \mu_i = \alpha + \beta_1 x_i \\
\\
\mathcal{M}_2 : \,& h_i \sim \text{Normal}(\mu_i, \sigma) \\
& \mu_i = \alpha + \beta_1 x_i + \beta_2 x_i^2\\
\\
\mathcal{M}_3 : \,& h_i \sim \text{Normal}(\mu_i, \sigma) \\
& \mu_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3   \\
\\
\mathcal{M}_4 :\, & h_i \sim \text{Normal}(\mu_i, \sigma) \\
& \mu_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3  + \beta_4 x_i^4  \\
\\
\mathcal{M}_5 : \,& h_i \sim \text{Normal}(\mu_i, \sigma) \\
& \mu_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3  + \beta_4 x_i^4 + \beta_5 x_i^5 \\
\\
\mathcal{M}_6 : \,& h_i \sim \text{Normal}(\mu_i, \sigma) \\
& \mu_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3  + \beta_4 x_i^4 + \beta_5 x_i^5 + \beta_6 x_i^6
\end{align*}$$

We fit the models using `map` using a weakly regularizing prior. What is a weakly regularizing prior? This [article](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) states that a weakly informative prior "should contain enough information to regularize: the idea is that the prior rules out unreasonable parameter values but is not so strong as to rule out values that might make sense". A helpful example on how to get a weakly informative prior has been this [article](http://mc-stan.org/users/documentation/case-studies/weakly_informative_shapes.html).

Since we centered and standardized age, the $\beta$ coefficients correspond to a decrease in centimeter if age increases by one standard deviation (about 20 years). So the question is, how much to we expect someone to grow in 20 years? Maybe up to 100cm if they're still young (e.g. less than 10years), otherwise maybe around 5cm. As a compromise, I picked $\text{Normal}(0,50)$ as a prior.
```{r, fig.height=8}
mod1 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*age,
    a ~ dnorm(140, 10),
    b1 ~ dnorm(0, 50),
    sigma ~ dunif(0, 40)
  ), data=d1
)

mod2 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*age +b2*age^2,
    a ~ dnorm(140, 10),
    c(b1, b2) ~ dnorm(0, 50),
    sigma ~ dunif(0, 40)
  ), data=d1
)

mod3 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*age +b2*age^2 + b3*age^3,
    a ~ dnorm(140, 10),
    c(b1, b2,b3) ~ dnorm(0, 50),
    sigma ~ dunif(0, 40)
  ), data=d1
)

mod4 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4 ,
    a ~ dnorm(140, 10),
    c(b1,b2,b3,b4) ~ dnorm(0, 50),
    sigma ~ dunif(0, 40)
  ), data=d1
)

mod5 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4 + 
      b5*age^5 ,
    a ~ dnorm(140, 10),
    c(b1,b2,b3,b4,b5) ~ dnorm(0, 50),
    sigma ~ dunif(0, 40)
  ), data=d1
)

mod6 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4 + 
      b5*age^5 + b6*age^6,
    a ~ dnorm(140, 10),
    c(b1,b2,b3,b4,b5,b6) ~ dnorm(0, 50),
    sigma ~ dunif(0, 40)
  ), data=d1
)

plot(coeftab( mod1, mod2, mod3, mod4, mod5, mod6), 
     pars=c("b1", "b2", "b3", "b4", "b5", "b6"))
```

The parameters for the beta coefficients vary quite a bit and are in between -20 and +20.

__6H1.__ Compare the models above, using WAIC. Compare the models ranking, as well as the WAIC weights.
```{r}
( height.models <- compare(mod1, mod2, mod3, mod4, mod5, mod6) )
```

```{r}
plot( height.models )
```

The fourth model has the lowest WAIC but the fifth and sixth model have basically almost indistinguishably slightly higher WAICs. The first and second model on the other hand have much higher WAIC. Thus all the Akaike weight is on the fourth, fifth and sixth model.

__6H2.__ For each model, produce a plot with model averaged mean and 97% confidence interval of the mean, superimposed on the raw data.
```{r}
plot_model <- function(mod){
  name <- deparse(substitute(mod))
  age.seq <- seq(from=-3, to=3, length.out = 30)    # generate ages (centered and scaled)
  post <- extract.samples(mod)                     # extract a posterior sample
  
  num_coef <- ncol(post) - 2
  # compute mu
  mu <- link( mod, data=data.frame(age=age.seq) )
  mu.mean <- apply(mu, 2, mean)
  mu.HPDI <- apply(mu, 2, HPDI, prob=0.97)

  # compute predicted height
  # sim.height <- sim( mod, data=list(age=age.seq ))
  
  # height.HPDI <- apply(sim.height, 2, HPDI, prob=0.89)
  # height.mean <- apply(sim.height, 2, mean)

  # plot everything
  plot(height ~ age, data=d1, col=col.alpha("black", 0.5), ylim=c(50, 180))   # train data in black
  points( height ~ age, data=d2, col=col.alpha(rangi2, 0.8))     # test data in blue
  lines( age.seq, mu.mean)                                      # the MAP regression line
  shade( mu.HPDI, age.seq)                               # draw HPDI region around the regression line
  #shade( height.HPDI, age.seq)                          # draw HPDI region for the simulated heights
  
  legend("bottomright", c("Train", "Test"), pch=c(1,1), col=c("black", rangi2), bty="n")
  mtext(name)
}
```

```{r, fig.height=15, fig.width=10, message=F, warning=F}
par(mfrow=c(3,2))
plot_model(mod1)
plot_model(mod2)
plot_model(mod3)
plot_model(mod4)
plot_model(mod5)
plot_model(mod6)
```

Both the first and second model have an inflexible model that does not fit the data well. The only discernible difference in the last three models is in the way the model predicts outside of the data. This is also the part where we can see that the model follows more closely the training data.

__6H3.__ Plot the model averaged predictions, across all models. In what ways do the averaged predictions differ from the predictions of the model with the lowest WAIC value?
```{r, warning=F, message=F, fig.height=6, fig.width=10}
age.seq <- seq(from=-3, to=3, length.out = 30)
height.ensemble <- ensemble( mod1, mod2, mod3, mod4, mod5, mod6, 
                             data=data.frame(age=age.seq ) )

# compute mu
mu.mean <- apply(height.ensemble$link, 2, mean)
mu.HPDI <- apply(height.ensemble$link, 2, HPDI, prob=0.97)

par(mfrow=c(1,2))
# plot everything
plot(height ~ age, data=d1, col=col.alpha("black", 0.5), ylim=c(50, 180))   # train data in black
points( height ~ age, data=d2, col=col.alpha(rangi2, 0.8))     # test data in blue
lines( age.seq, mu.mean)                                      # the MAP regression line
shade( mu.HPDI, age.seq)                              # draw HPDI region around the regression line
#shade( height.HPDI, age.seq)                         # draw HPDI region for the simulated heights

legend("bottomright", c("Train", "Test"), pch=c(1,1), col=c("black", rangi2), bty="n")
mtext("Averaged model")

plot_model(mod4)
```

Since only the models 4, 5, and 6 had some positive weight, we do not see any influence of the other three models in the averaged predictions. Compared with model 4 (the one with the lowest WAIC), the averaged model places more uncertainty at the higher ends of the data.

__6H4.__ Compute the test-sample deviance for each model. We can compute the deviance for each model as follow:
```{r}
compute_deviance <- function(mod, data) {
  mu <- link( mod, data=data )
  mu.mean <- apply(mu, 2, mean)
  sigma <- coef(mod)["sigma"]
  
  logLikelihood <- sum( dnorm( data$height, 
            mu.mean,
            sigma, 
            log=TRUE))
  deviance <- (-2) * logLikelihood
  deviance
}
```

The test-sample deviance for the models are thus:
```{r}
dev <- sapply(list(mod1, mod2, mod3, mod4, mod5, mod6), compute_deviance, data=d2)
names(dev) <- c("mod1", "mod2", "mod3", "mod4", "mod5", "mod6")
dev
```


__6H5.__ Compare the deviances to the WAIC values. To make the values easier to compare, we center both by subtracting the minimum.
```{r}
dev.c <- dev - min(dev)
waic <- compare(mod1, mod2, mod3, mod4, mod5, mod6, sort=NULL)
waic <- attr(waic, "output")$WAIC
waic.c <- waic - min(waic)

```

```{r}
plot(1:6 , waic.c, ylim=c(0,max(c( waic.c, dev.c))),
       xlab="Model", ylab="centered estimate", 
      pch=16, cex=1, col="steelblue", type="b",
     main="WAIC Estimates vs Test Deviance")
points(1:6, dev.c, cex=1, pch=16, type="b")

legend("topright", c( "WAIC", "Test Deviance"), pch=c(16, 16), 
       lwd=c(1,1), col=c("steelblue", "black"), bty="n")
```

Both the WAIC and the test deviance agree which models best predict the data: model 4, 5, and 6. In this case, WAIC does a quite good job in approximating the test deviance.

__6H6:__ Consider the following model:
$$\begin{align*}
h_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \beta_4 x_i^4 + \beta_5 x_i^5 + \beta_6 x_i^6 \\
\beta_i &\sim \text{Normal}(0, 5) 
\end{align*}$$
and assume flat (or nearly flat) priors on $\alpha$ and $\sigma$.

This model has more strongly regularizing priors on the beta coefficients.
Fit the model to `d1`:
```{r}
mod <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4 + 
          b5*age^5 + b6*age^6,
    a ~ dnorm(140, 50),
    c(b1,b2,b3,b4,b5,b6) ~ dnorm(0, 5),
    sigma ~ dunif(0, 50)
  ), data=d1
)

precis(mod)
```
```{r}
plot(precis(mod))
```

The beta parameter values are all much smaller than the parameter values in the models before.
```{r}
WAIC(mod)
```
```{r}
waic
```
The model has a WAIC of 1930 which gives it a similar (though slightly higher) WAIC than the best performing models from before. The effective number of parameter for the regularized model is 6.4.

We plot the predictions together with the confidence interval of the mean. For comparison, I also plot the model 4 (with the best WAIC) from before.
```{r, fig.height=6, fig.width=10}
par(mfrow=c(1,2))
plot_model(mod)
plot_model(mod4)
```

The curves are both very similar, though the regularized model seems to have a slightly less wiggly curve. Also, it puts more uncertainty to the higher ages.

Let's compute the deviance:
```{r}
compute_deviance(mod, data=d2)
```
```{r}
dev
```
The regularized model has an out-of-sample deviance even lower than the best deviance from the models before. The regularizing prior thus produce a better model, with better predictive power than the models with less regularizing prior.
