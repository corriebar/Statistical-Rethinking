---
title: "Binomial Regression"
author: "Corrie"
date: "October 4, 2018"
output: 
  html_document:
    toc: true
    toc_float: true
  github_document:
    pandoc_args: --webtex 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA)
```

## Logistic Regression
The chimpanzee data: Do chimpanzee pick the more social option?
```{r, message=F, warning=F}
library(rethinking)
data(chimpanzees)
d <- chimpanzees
```

The important variables are the variable `condition`, indicating if another chimpanzee sits opposite (1) the table or not (0) and the variable `prosocial_left` which indicates if the left lever is the more social option.  These two variables will be used to predict if the chimpanzees pull the left lever or not (`pulled_left`).

The implied model is:
$$\begin{align*}
L_i &\sim \text{Binomial}(1, p_i)\\
\text{logit}(p_i) &= \alpha + (\beta_P + \beta_{PC}C_i)P_i \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta_P &\sim \text{Normal}(0, 10) \\
\beta_{PC} &\sim \text{Normal}(0, 10) 
\end{align*}$$

We'll write this as a `map()` model but will start with two simpler models with less predictors, starting with one model that has only an intercept and no predictor variables:
```{r}
m10.1 <- map(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a,
    a ~ dnorm(0, 10)
  ),
  data=d
)
precis(m10.1)
```

This implies a MAP probability of pulling the left lever of
```{r}
logistic(0.32)
```
with a 89% interval of
```{r}
logistic( c(0.18, 0.46))
```

Next the models using the predictors:
```{r}
m10.2 <- map(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a + bp*prosoc_left,
    a ~ dnorm(0, 10),
    bp ~ dnorm(0, 10)
  ),
  data=d
)
m10.3 <- map(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a + (bp + bpC*condition)*prosoc_left,
    a ~ dnorm(0, 10),
    bp ~ dnorm(0, 10),
    bpC ~ dnorm(0, 10)
  ),
  data=d
)

(cp <- compare(m10.1, m10.2, m10.3) )
```
The model without the interaction seems to fare better even though the third model best reflects the structure of the experiment.
```{r, fig.height=2.2, fig.width=8}
plot( cp , 
      xlim=c( min( cp@output$WAIC - cp@output$SE) , 
              max(cp@output$WAIC + cp@output$SE ) ) )
```

Note also that the difference has a small standard error, so the order of the models wouldn't easily change.

Let's have a look at the third model to understand why it performs poorly compared with the second one:
```{r}
precis(m10.3)
```

```{r, fig.height=2.2, fig.width=8}
plot( precis(m10.3) )
```

The interaction variable has a rather wide posterior.
Let's have a closer look at the parameter `bp` for the effect of the prosocial option. We have to distinguish between the absolute and relative effect.

Changing the predictor `prosoc_left` from 0 to 1 increases the log-odds of pulling the left-hand lever by 0.61. This implies the odds are multiplied by:
```{r}
exp(0.61)
```
That is, the odds increase by 84%. This is the relative effect. The relative effect depend strongly on the other parameter als well. If we assume for example an $\alpha$ value of 4 then the probability for a pull, ignoring everything else:
```{r}
logistic(4)
```
is already quite high. Adding then the increase of 0.61 (relative increase of 84%) changes this to
```{r}
logistic(4 + 0.61)
```
In this example, the absolute effect would thus be small.

Let's plot the absolute effect:
```{r, results='hide', warning=F, message=F}
# dummy data for predictions across treatments
d.pred <- data.frame(
  prosoc_left = c(0, 1, 0, 1), # right/left/right/left
  condition = c(0, 0, 1, 1)    # control/control/partner/partner
)

# build prediction ensemble
chimp.ensemble <- ensemble(m10.1, m10.2, m10.3, data=d.pred)

# summarize
pred.p <- apply(chimp.ensemble$link, 2, mean)
pred.p.PI <- apply(chimp.ensemble$link, 2, PI)
```

```{r, fig.height=5, fig.width=5}
plot( 0, 0, type="n", xlab="prosoc_left/condition",
      ylab="proportion pulled left",
      ylim=c(0,1), xaxt="n", xlim=c(1,4) )
axis(1, at=1:4, labels=c("0/0", "1/0", "0/1", "1/1"))

p <- by( d$pulled_left,
    list(d$prosoc_left, d$condition, d$actor ), mean)

for ( chimp in 1:7) 
  lines( 1:4, as.vector(p[,,chimp]),
         col="royalblue4", lwd=1.5)

lines( 1:4, pred.p )
shade(pred.p.PI, 1:4)
```

Compare the MAP model with a MCMC Stan model:
```{r, results="hide", warning=F, message=F}
# clean NAs from the data
d2 <- d
d2$recipient <- NULL

# re-use map fit to get the formula
m10.3stan <- map2stan(m10.3, data=d2, iter=1e4, warmup=1000 )
precis(m10.3stan)
```

```{r, warning=F}
precis(m10.3stan)
```

The numbers are almost identical with the MAP model and we can check the pairs plot to see that the posterior is indeed well approximated by a quadratic.
```{r, fig.height=6, fig.width=7}
pairs(m10.3stan)
```

We saw in the plot of the average proportions pulled left above that some chimpanzees have a preference for pulling the left or right lever. One even always pulled the left lever. A factor here might be the handedness of the chimp. 
One thing we can do, is to fit an intercept for each individula:
$$\begin{align*}
L_i &\sim \text{Binomial}(1, p_i)\\
\text{logit}(p_i) &= \alpha_{\text{ACTOR}[i]} + (\beta_P + \beta_{PC}C_i)P_i \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta_P &\sim \text{Normal}(0, 10) \\
\beta_{PC} &\sim \text{Normal}(0, 10) 
\end{align*}$$
We fit this with MCMC since it will turn out to have some skew in its posterior distribution.
```{r, results='hide', warning=F, message=F}
m10.4 <- map2stan(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + (bp + bpC*condition)*prosoc_left,
    a[actor] ~ dnorm(0, 10),
    bp ~ dnorm(0, 10),
    bpC ~ dnorm(0, 10)
  ),
  data=d2, chains=2, iter=2500, warmup=500
)
```

Number of actors:
```{r}
unique(d$actor)
```

```{r, warning=F, message=F}
precis(m10.4, depth=2)
```
This posterior is not entirely Gaussian:
```{r}
post <- extract.samples( m10.4)
str(post)
```
```{r}
dens(post$a[,2])
```

Plotting the predictions for each actor:
```{r, results='hide', fig.height=14, fig.width=8}
par(mfrow=c(4, 2))
for (chimp in 1:7) {
  d.pred <- list(
    pulled_left = rep(0, 4),       # empty outcomes
    prosoc_left = c(0, 1, 0, 1),   # right/left/right/left
    condition = c(0, 0, 1, 1),     # control/control/partner/partner
    actor = rep(chimp, 4)
  )
  link.m10.4 <- link( m10.4, data=d.pred)
  pred.p <- apply( link.m10.4, 2, mean)
  pred.p.PI <- apply( link.m10.4, 2, PI )
  
  plot(0, 0, type="n", xlab="prosoc_left/condition",
       ylab="proportion pulled left",
       ylim=c(0,1), xaxt="n",
       xlim=c(1, 4), yaxp=c(0,1,2) )
  axis(1, at=1:4, labels=c("0/0", "1/0", "0/1", "1/1"))
  mtext(paste( "actor", chimp ))
  
  p <- by( d$pulled_left,
           list(d$prosoc_left, d$condition, d$actor), mean )
  lines( 1:4, as.vector(p[,,chimp]), col="royalblue4", lwd=2)
  
  lines(1:4, pred.p)
  shade(pred.p.PI, 1:4)
}
```

## Aggregated binomial
### Chimpanzees

Instead of predicting the likelihood for a single pull (0 or 1), we can also predict the counts, i.e. how likely is it that an actor pulls left `x` times out of 18 trials.
```{r}
data(chimpanzees)
d <- chimpanzees
d.aggregated <- aggregate( d$pulled_left,
                           list(prosoc_left=d$prosoc_left,
                                condition=d$condition,
                                actor=d$actor),
                           sum)
knitr::kable( head(d.aggregated,  8) )
```

```{r}
m10.5 <- map(
  alist(
    x ~ dbinom( 18, p ),
    logit(p) <- a + (bp + bpC*condition)*prosoc_left,
    a ~ dnorm(0, 10),
    bp ~ dnorm(0, 10),
    bpC ~ dnorm(0, 10)
  ),
  data = d.aggregated
)
precis(m10.5)
```

Compare with the same model with non-aggregated data:
```{r}
precis(m10.3stan)
```

We get the same estimates.

### Graduate school admissions
```{r}
data("UCBadmit")
d <- UCBadmit
knitr::kable(d)
```

The data set contains only 12 rows but since it is aggregated, it actually represents `r sum(d$applications)` applications.
The goal is to evaluate whether the data contains evidence for gender bias in admission.

We will fit two models: One using gender as a predictor for `admit` and one modelling `admit` as a constant, ignoring gender.
```{r}
d$male <- ifelse( d$applicant.gender == "male", 1, 0 )

m10.6 <- map(
  alist(
    admit ~ dbinom( applications, p ),
    logit(p) <- a + bm*male,
    a ~ dnorm(0, 10),
    bm ~ dnorm(0, 10)
  ),
  data=d
)

m10.7 <- map(
  alist(
    admit ~ dbinom( applications, p),
    logit(p) <- a,
    a ~ dnorm(0, 10)
  ),
  data=d
)
```


```{r}
(cp <- compare( m10.6, m10.7 ) )
```

The model using male as a predictor performs better than without. The comparison suggests that gender actually matters a lot:
```{r}
plot( cp , 
      xlim=c( min( cp@output$WAIC - cp@output$SE) , 
              max(cp@output$WAIC + cp@output$SE ) ) )
```

In which way does it matter?
```{r}
precis( m10.6 )
```

Being male does improve the chances of being admitted.
The relative difference is `exp(0.61) = ` `r exp(0.61)`. This means that a male applicant's odds are 184% of a female applicant.
Let's get the absolute scale, which is more important:
```{r}
post <- extract.samples( m10.6 )
p.admit.male <- logistic( post$a + post$bm )
p.admit.female <- logistic( post$a )

diff.admit <- p.admit.male - p.admit.female

quantile( diff.admit, c(0.025, 0.5, 0.975 ))
```

Thus the median estimate of the male advantage is about 14%.
```{r}
dens( diff.admit )
```


```{r, results='hide'}
postcheck( m10.6, n=1e4 , col="royalblue4")

# draw lines connecting points from same dept
for ( i in 1:6 ){
  x <- 1 + 2*(i-1)
  y1 <- d$admit[x]/d$applications[x]       # male
  y2 <- d$admit[x+1]/d$applications[x+1]   # female
  lines( c(x, x+1), c(y1, y2), col="royalblue4", lwd=2)
  text(x+0.5, (y1 + y2)/2 + 0.05, d$dept[x], cex=0.8, col="royalblue4")
}
```

The first point of a line is the admission rate for males while the second point of a line is the admission rate for females. The expeted predictions are the black open points and the black crosses indicate the 89% interval of the expectations.
The plot shows that only two departments (C and E) and lower admission rates for females.
How come we get such a bad prediction?

The problem: Male and female applicants don't apply to the same departments and departments have different admission rates. Department A has much higer admission rates than department F and female applicants apply more often to F than to A.

We will build a model that uses a unique intercept per department.
```{r}
# make index
d$dept_id <- coerce_index( d$dept )

# model with unique intercept for each dept
m10.8 <- map(
  alist(
    admit ~ dbinom( applications, p),
    logit(p) <- a[dept_id],
    a[dept_id] ~ dnorm(0, 10)
  ),
  data=d
)

m10.9 <- map(
  alist(
    admit ~ dbinom( applications, p),
    logit(p) <- a[dept_id] + bm*male,
    a[dept_id] ~ dnorm(0, 10),
    bm ~ dnorm(0, 10)
  ),
  data=d
)
```

We then compare all three models:
```{r}
(cp <- compare( m10.6, m10.7, m10.8, m10.9 ) )
```

As a plot:
```{r}
plot( cp , 
      xlim=c( min( cp@output$WAIC - cp@output$SE) , 
              max(cp@output$WAIC + cp@output$SE ) ) )
```

The two new models with the unique intercepts perform much better. Now, the model without `male` is ranked first but the difference between the first two models is tiny. Both models got about half the Akaike weight so you could call it a tie.

So how does gender now affect admission?
```{r}
precis( m10.9, depth=2 )
```

The estimate for `bm` has changed direction, meaning it now estimates that being male is a disadvantage! The estimate becomes `exp(-0.1) = ` `r exp(-0.1)`. So a male has about 90% the odds of admission as a female.

Let's do the posterior check again as before:
```{r, results='hide'}
postcheck( m10.9, n=1e4 , col="royalblue4")

# draw lines connecting points from same dept
for ( i in 1:6 ){
  x <- 1 + 2*(i-1)
  y1 <- d$admit[x]/d$applications[x]       # male
  y2 <- d$admit[x+1]/d$applications[x+1]   # female
  lines( c(x, x+1), c(y1, y2), col="royalblue4", lwd=2)
  text(x+0.5, (y1 + y2)/2 + 0.05, d$dept[x], cex=0.8, col="royalblue4")
}
```

The predictions fit much better than before.

Let's also check the quadratic approximation. In the example with the chimpanzees, unique intercepts were a problem for quadratic approximations, so let's check how the compare to a Stan model:
```{r, results='hide', warning=F, message=F}
dstan <- d[, c("admit", "applications", "male", "dept_id")]
m10.9stan <- map2stan( m10.9, data=dstan,
                       chains=2, iter=2500, warmup=500)
precis(m10.9stan, depth=2)
```

```{r, fig.height=8, fig.width=9}
pairs(m10.9stan)
```

All the posterior distributions are pretty much Gaussian, a quadratic approximation thus gives good estimates.

## Fitting binomial regression with `glm`

The following code yields similar results as the map approach for the aggregated binomial.
```{r}
m10.7glm <- glm( cbind( admit, reject) ~ 1, data=d, family=binomial)
m10.6glm <- glm( cbind( admit, reject) ~ male, data=d, family=binomial)
m10.8glm <- glm( cbind( admit, reject) ~ dept, data=d, family=binomial)
m10.9glm <- glm( cbind( admit, reject) ~ male + dept, data=d, 
                 family=binomial )
precis(m10.9glm)
```

Compare with the `map()` model:
```{r}
precis(m10.9stan, depth=2)
```
Note that the departments are coded differently: the intercept in the `glm` model corresponds to `a[1]` in the Stan model and `deptB` in the `glm` model corresponds to the difference from department A to B, that is `a[2]-a[1]` in the Stan model.

To use `glm()` for a non-aggregated model:
```{r}
m10.4glm <- glm(
  pulled_left ~ as.factor(actor) + prosoc_left*condition  -condition,
  data=chimpanzees, family=binomial
)
precis(m10.4glm)
```

We need to use `-condition` to remove the main effect of condition.

We can also use `glimmer()` to get code corresponding to a `map` or `map2stan` model.
```{r}
glimmer( pulled_left ~ prosoc_left*condition -condition,
         data=chimpanzees, family=binomial)
```

Note that `glm` uses flat priors by default which can lead to nonsense estimates. Consider for example the following example:
```{r}
# almost perfectly associated
y <- c( rep(0, 10), rep(1, 10))
x <- c(rep(-1, 9), rep(1, 11))
m.bad <- glm( y ~ x, data=list(x=x, y=y), family=binomial)
precis(m.bad)
```

The estimates would suggest there is no association between `x` and `y` even though there is a strong association. A weak prior helps:
```{r}
m.good <- map(
  alist(
    y ~ dbinom(1, p),
    logit(p) <- a + b*x,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10)
  ),
  data=list(x=x, y=y)
)
precis(m.good)
```

Since the uncertainty is not symmetric in this case, the quadratic assumption is misleading. Even better would be MCMC samples:
```{r, results='hide', fig.height=5, fig.width=6, warning=FALSE, message=FALSE}
m.better <- map2stan( m.good)
pairs(m.better)
```

