---
title: "Chapter 4 - Exercises"
author: Corrie
date: "2020-04-22"
slug: chp4-ex
layout: "single-projects"
categories:
  - R
  - Statistical Rethinking
tags: 
  - Statistical Rethinking
  - Bayesian 
comments: yes
image: 'images/tea_with_books.jpg'
share: yes
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>


<p>These are my solutions to the practice questions of chapter 4, <em>Linear Models</em>, of the book “Statistical Rethinking” (version 2) by Richard McElreath.</p>
<div id="easy." class="section level2">
<h2>Easy.</h2>
<p><strong>4E1.</strong> In the model definition below, which line is the likelihood:
<span class="math display">\[
\begin{align*}
y_i &amp;\sim \text{Normal}(\mu, \sigma) &amp; &amp; \text{This is the likelihood}\\
\mu &amp;\sim \text{Normal}(0, 10) \\
\sigma &amp;\sim \text{Exponential}(1)
\end{align*} \]</span></p>
<p><strong>4E2.</strong> In the model definition just above, how many parameters are in the posterior distribution?</p>
<p>There are <strong>2</strong> parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<p><strong>4E3.</strong> Write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.</p>
<p><span class="math display">\[\begin{align*}
P(\mu, \sigma| y_i) &amp;\propto \text{Likelihood } \times \text{ Prior probability} \\
&amp; \propto \mathcal{L}(y | \mu, \sigma ) \times P(\mu) \times P(\sigma)
\end{align*}\]</span>
For the likelihood, we get the following term:
<span class="math display">\[\begin{align*}
\mathcal{L}(y | \mu, \sigma ) &amp;= \prod_i \text{Normal}(y_i|\mu, \sigma) \\
&amp; = \prod_i \frac{1}{\sqrt{2\pi \sigma^2} }\exp(- \frac{(y_i-\mu)^2}{2\sigma^2}).
\end{align*}\]</span>
and for the two priors we have
<span class="math display">\[\begin{align*}
P(\mu) &amp;= \text{Normal}(\mu| 0,10) \\
&amp;= \frac{1}{\sqrt{2\pi \times 10^2} }\exp(- \frac{\mu^2}{2\times 10^2}) \\
\\
P(\sigma) &amp;= \text{Exponential}(\sigma|1) \\
&amp;= e^{-\sigma}.
\end{align*}\]</span>
Plugging everything in one equation, we get the following:
<span class="math display">\[\begin{align*}
P(\mu, \sigma| y_i) &amp;= \frac{\prod_i \text{Normal}(y_i|\mu, \sigma) \times 
\text{Normal}(\mu| 0,10) \times \text{Exponential}(\sigma|1) }
{\int \prod_i \text{Normal}(y_i|\mu, \sigma) \times 
\text{Normal}(\mu| 0,10) \times \text{Exponential}(\sigma|1) \text{ d}\mu\text{d}\sigma} \\
\\
&amp;\propto \prod_i \frac{1}{\sqrt{2\pi \sigma^2} }\exp(- \frac{(y_i-\mu)^2}{2\sigma^2}) \times 
\frac{1}{\sqrt{2\pi \times 10^2} }\exp(- \frac{\mu^2}{2\times 10^2}) \times  e^{-\sigma}.
\end{align*}\]</span></p>
<p>This big construct in the last line is then basically one big function that takes <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> as input where <span class="math inline">\(y_i\)</span>, our data, is given (and thus fixed).</p>
<p><strong>4E4.</strong> In the model definition below, which line is the linear model?
<span class="math display">\[\begin{align*}
y_i &amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp;= \alpha + \beta x_i &amp; \text{This is the linear model}\\
\alpha &amp;\sim \text{Normal}(0,10) \\
\beta &amp;\sim \text{Normal}(0,1) \\
\sigma &amp;\sim \text{Exponential}(2)
\end{align*}\]</span></p>
<p><strong>4E5.</strong> In the model definition just above, how many parameters are in the posterior distribution?</p>
<p>There are <strong>3</strong> parameters in the posterior distribution, <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma\)</span>.</p>
</div>
<div id="medium." class="section level2">
<h2>Medium.</h2>
<p><strong>4M1.</strong> For the model definition below, simulate observed heights from the prior (not the posterior).
<span class="math display">\[\begin{align*}
y_i &amp;\sim \text{Normal}(\mu, \sigma) \\
\mu &amp;\sim \text{Normal}(0, 10) \\
\sigma &amp;\sim \text{Exponential}(1)
\end{align*}\]</span></p>
<pre class="r"><code>n &lt;- 10000
mu &lt;- rnorm(n, 0, 10)
sigma &lt;- rexp(n, 2)
y_prior &lt;- rnorm(n, mu, sigma)
dens(y_prior)</code></pre>
<p><img src="/projects/Statistical-Rethinking/Chapter_4/chapter4_Ex_files/figure-html/unnamed-chunk-2-1.svg" width="480" /></p>
<p><strong>4M2.</strong> Translate the model just above into a <code>quap()</code> formula.</p>
<pre class="r"><code>flist &lt;- alist(
            y ~ dnorm(mu, sigma),
            mu ~ dnorm(0, 10),
            sigma ~ dexp(1)
          ) </code></pre>
<p><strong>4M3.</strong> Translate the <code>quap()</code> formula below into a mathematical model definition.</p>
<pre class="r"><code>flist &lt;- alist(
  y ~ dnorm( mu, sigma ),
  mu &lt;- a + b*x,
  a ~ dnorm( 0, 10 ),
  b ~ dnorm( 0, 1 ),
  sigma ~ dexp( 1 )
)</code></pre>
<p>The mathematical definition:
<span class="math display">\[\begin{align*}
y_i &amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp;= \alpha + \beta x_i \\
\alpha &amp;\sim \text{Normal}(0,10) \\
\beta &amp;\sim  \text{Normal}(0,1) \\
\sigma &amp;\sim \text{Exponential}(1) 
\end{align*}\]</span></p>
<p><strong>4M4.</strong> A sample of students is measured for height each year for three years. You want to fit a linear regression, using year as a prediction. Write down the mathematical model definition.
<span class="math display">\[\begin{align*}
h_i &amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp;= \alpha + \beta t_i \\
\alpha &amp;\sim \text{Normal}(120, 20) \\
\beta &amp;\sim \text{Normal}(0, 10) \\
\sigma &amp;\sim \text{Uniform}(0, 50)
\end{align*}\]</span>
Here, <span class="math inline">\(h_i\)</span> is the height and <span class="math inline">\(t_i\)</span> is the year of the <span class="math inline">\(i\)</span>th observation. Since <span class="math inline">\(\alpha\)</span> is the average height of a student at year zero, I picked a normal distribution with mean 120 (assuming an average height of 120cm) and standard deviation 20. For <span class="math inline">\(\beta\)</span>, I picked a normal distribution with mean 0 and standard deviation 10, meaning on average, a person grows 0cm per year with standard deviation 10cm, since I don’t expect many people to grow or shrink more than 20cm per year. For <span class="math inline">\(\sigma\)</span>, I picked a uniform distribution over the interval <span class="math inline">\([0, 50]\)</span>, expecting that the variance among heights for students of the same age is not larger than 50cm.</p>
<p>We can also do a small prior predictive check:</p>
<pre class="r"><code>N &lt;- 1000
a &lt;- rnorm( N, 120, 20 )
b &lt;- rnorm( N, 0, 10 )
sigma &lt;- runif( N, 0, 50 )</code></pre>
<p><img src="/projects/Statistical-Rethinking/Chapter_4/chapter4_Ex_files/figure-html/unnamed-chunk-6-1.svg" width="480" /></p>
<p>The model mostly stays between 0cm and 250cm. It still goes quite extreme and also has negative growth but otherwise it doesn’t look to unreasonable.</p>
<p>A short look at the predicted distribution of height at the first year:</p>
<pre class="r"><code>height_0 &lt;- rnorm( N, a + b * 0, sigma )
dens( height_0 )</code></pre>
<p><img src="/projects/Statistical-Rethinking/Chapter_4/chapter4_Ex_files/figure-html/unnamed-chunk-7-1.svg" width="480" /></p>
<p>Most students at first year would be expected to be between 50cm and 200cm tall.</p>
<p><strong>4M5.</strong> Now suppose, I remind you that every student got taller each year. I will change my priors as follows:
<span class="math display">\[\begin{align*}
\alpha &amp;\sim \text{Normal}(120, 20) \\
\beta &amp;\sim \text{Log-Normal}(0, 2.5)
\end{align*}\]</span>
I changed <span class="math inline">\(\beta\)</span> to a log-normal distribution, so that <span class="math inline">\(\beta\)</span>, the indicator for growth per year, is greater or equal than zero.
The resulting model lines then look as follows:
<img src="/projects/Statistical-Rethinking/Chapter_4/chapter4_Ex_files/figure-html/unnamed-chunk-8-1.svg" width="480" /></p>
<p><strong>4M6.</strong> Now suppose, the variance among heights for students of the same age is never more than 64cm. I thus change my priors as follows:
<span class="math display">\[\sigma \sim \text{Uniform}(0, 64).\]</span>
<strong>4M7.</strong> Refit model <code>m4.3</code> from the chapter but omit the mean weight <code>xbar</code>. Compare the new model’s posterior to that of the original model. In particular, look at the covariance among the parameters. What is different?</p>
<p>First, let’s load the data and fit the <code>m4.3</code> model again:</p>
<pre class="r"><code>data(&quot;Howell1&quot;)
d &lt;- Howell1
d2 &lt;- d[ d$age &gt;= 18, ]
xbar &lt;- mean( d2$weight )
m4.3 &lt;- quap(
        alist(
          height ~ dnorm( mu, sigma),
          mu &lt;- a + b * ( weight - xbar ) ,
          a ~ dnorm( 178, 20),
          b ~ dlnorm( 0, 1),
          sigma ~ dunif(0, 50)
        ) ,
        data = d2
)
precis(m4.3)</code></pre>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
5.5%
</th>
<th style="text-align:right;">
94.5%
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
154.601
</td>
<td style="text-align:right;">
0.270
</td>
<td style="text-align:right;">
154.169
</td>
<td style="text-align:right;">
155.03
</td>
</tr>
<tr>
<td style="text-align:left;">
b
</td>
<td style="text-align:right;">
0.903
</td>
<td style="text-align:right;">
0.042
</td>
<td style="text-align:right;">
0.836
</td>
<td style="text-align:right;">
0.97
</td>
</tr>
<tr>
<td style="text-align:left;">
sigma
</td>
<td style="text-align:right;">
5.072
</td>
<td style="text-align:right;">
0.191
</td>
<td style="text-align:right;">
4.766
</td>
<td style="text-align:right;">
5.38
</td>
</tr>
</tbody>
</table>
<p>Now, we refit an uncentered version of the model:</p>
<pre class="r"><code>m4.3_u &lt;- quap(
        alist(
          height ~ dnorm( mu, sigma),
          mu &lt;- a + b * weight ,
          a ~ dnorm( 178, 20),
          b ~ dlnorm( 0, 1),
          sigma ~ dunif(0, 50)
        ) ,
        data = d2
)
precis(m4.3_u)</code></pre>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
5.5%
</th>
<th style="text-align:right;">
94.5%
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
114.534
</td>
<td style="text-align:right;">
1.898
</td>
<td style="text-align:right;">
111.501
</td>
<td style="text-align:right;">
117.567
</td>
</tr>
<tr>
<td style="text-align:left;">
b
</td>
<td style="text-align:right;">
0.891
</td>
<td style="text-align:right;">
0.042
</td>
<td style="text-align:right;">
0.824
</td>
<td style="text-align:right;">
0.957
</td>
</tr>
<tr>
<td style="text-align:left;">
sigma
</td>
<td style="text-align:right;">
5.073
</td>
<td style="text-align:right;">
0.191
</td>
<td style="text-align:right;">
4.767
</td>
<td style="text-align:right;">
5.378
</td>
</tr>
</tbody>
</table>
<p>The estimates for <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> are still the same, only <span class="math inline">\(\alpha\)</span> is changed. This makes sense since now in the uncentered version, the meaning of <span class="math inline">\(\alpha\)</span> has changed. Before <span class="math inline">\(\alpha\)</span> was the average height for when $ x -{x}$ was 0, that is for observations where the weight is equal to the average weight. Now, <span class="math inline">\(\alpha\)</span> is the average height for the case that the weight is 0. As there are no people with a weight of zero, this <span class="math inline">\(\alpha\)</span> is harder to interpret.
We can compute <span class="math inline">\(\mu\)</span> in the uncentered version for when <span class="math inline">\(x\)</span> is the average weight:</p>
<pre class="r"><code>113.9 + 0.9 * xbar</code></pre>
<pre><code>[1] 154</code></pre>
<p>and unsurprisingly, we get the same value as in the centered model. The results for the two models are thus pretty much equal.
Let’s check the covariance between parameters. Remember, in the centered version, the correlation between parameters was practically zero.</p>
<pre class="r"><code>( vcm &lt;- vcov( m4.3_u ) )</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
a
</th>
<th style="text-align:right;">
b
</th>
<th style="text-align:right;">
sigma
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
3.60
</td>
<td style="text-align:right;">
-0.08
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:left;">
b
</td>
<td style="text-align:right;">
-0.08
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
sigma
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.04
</td>
</tr>
</tbody>
</table>
<p>We now observe some correlation between <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Let’s check the correlation matrix:</p>
<pre class="r"><code>cov2cor( vcm )</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
a
</th>
<th style="text-align:right;">
b
</th>
<th style="text-align:right;">
sigma
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
-0.99
</td>
<td style="text-align:right;">
0.03
</td>
</tr>
<tr>
<td style="text-align:left;">
b
</td>
<td style="text-align:right;">
-0.99
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
-0.03
</td>
</tr>
<tr>
<td style="text-align:left;">
sigma
</td>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
-0.03
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
</tbody>
</table>
<p>Now, there’s a quite strong negative correlation between the two parameters.</p>
<p>The same in visual:</p>
<p><img src="/projects/Statistical-Rethinking/Chapter_4/chapter4_Ex_files/figure-html/unnamed-chunk-16-1.svg" width="576" /></p>
<p>What is happening here? Every time the slope parameter increases a bit, the intercept changes in the opposite direction, i.e. decreases.</p>
<p>Compare the posterior predictions of both models:
<img src="/projects/Statistical-Rethinking/Chapter_4/chapter4_Ex_files/figure-html/unnamed-chunk-17-1.svg" width="960" /></p>
<p>The posterior predictions look very much the same for both models.</p>
<p><strong>4M8.</strong> In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots and observe what happens to the resulting spline.
[WIP]</p>
</div>
<div id="hard." class="section level2">
<h2>Hard.</h2>
<p><strong>4H1.</strong> !Kung census data: Provide predicted heights and 89% intervals for the following weights of individuals.</p>
<pre class="r"><code>weights &lt;- c(46.95, 43.72, 64.78, 32.59, 54.63)</code></pre>
<p>For this, I reuse the model <code>m4.3</code> from above and simulate heights for the individuals above by hand:</p>
<pre class="r"><code>post &lt;- extract.samples(m4.3)
sim.height &lt;- sapply( weights, function(weight) {
  rnorm(
    n = nrow(post),
    mean = post$a + post$b * ( weight - xbar ),
    sd = post$sigma
  )
})</code></pre>
Computing the mean and 89% compatibility interval using <code>PI()</code> gives us:
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
individual
</th>
<th style="text-align:right;">
weight
</th>
<th style="text-align:right;">
exptected_height
</th>
<th style="text-align:right;">
PI_89_lower
</th>
<th style="text-align:right;">
PI_89_upper
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
47.0
</td>
<td style="text-align:right;">
156
</td>
<td style="text-align:right;">
148
</td>
<td style="text-align:right;">
165
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
43.7
</td>
<td style="text-align:right;">
153
</td>
<td style="text-align:right;">
145
</td>
<td style="text-align:right;">
162
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
64.8
</td>
<td style="text-align:right;">
172
</td>
<td style="text-align:right;">
164
</td>
<td style="text-align:right;">
180
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
32.6
</td>
<td style="text-align:right;">
143
</td>
<td style="text-align:right;">
135
</td>
<td style="text-align:right;">
152
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
54.6
</td>
<td style="text-align:right;">
163
</td>
<td style="text-align:right;">
155
</td>
<td style="text-align:right;">
172
</td>
</tr>
</tbody>
</table>
<p><strong>4H2.</strong> Select the rows from the <code>Howell1</code> data with age below 18 years.</p>
<ol style="list-style-type: lower-alpha">
<li>Fit a linear regression to these data, using <code>quap()</code>.</li>
</ol>
<p>I will use the same model as above but adapt the prior for <span class="math inline">\(\alpha\)</span> to account for lower heights:</p>
<pre class="r"><code>d18 &lt;- d[ d$age &lt; 18, ]
xbar &lt;- mean( d18$weight )

model18 &lt;- quap(
  alist(
    height ~ dnorm( mu, sigma) ,
    mu &lt;- a + b * ( weight - xbar )  ,
    a ~ dnorm( 156, 20) ,
    b ~ dlnorm( 0, 1 ) ,
    sigma ~ dunif(0, 50)
  ),
  data=d18
)
precis(model18)</code></pre>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
5.5%
</th>
<th style="text-align:right;">
94.5%
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
108.36
</td>
<td style="text-align:right;">
0.609
</td>
<td style="text-align:right;">
107.39
</td>
<td style="text-align:right;">
109.34
</td>
</tr>
<tr>
<td style="text-align:left;">
b
</td>
<td style="text-align:right;">
2.72
</td>
<td style="text-align:right;">
0.068
</td>
<td style="text-align:right;">
2.61
</td>
<td style="text-align:right;">
2.83
</td>
</tr>
<tr>
<td style="text-align:left;">
sigma
</td>
<td style="text-align:right;">
8.44
</td>
<td style="text-align:right;">
0.431
</td>
<td style="text-align:right;">
7.75
</td>
<td style="text-align:right;">
9.12
</td>
</tr>
</tbody>
</table>
<p>As above, since the weight values are centered, the intercept <code>a</code> corresponds to the average height, which is here 108.3. This is much lower than in the model above (but expected since the individuals in this data set are younger). The slope <code>b</code> is interpreted such that for every 10kg heavier, an individual is expected to be 27cm taller. The standard deviation <code>sigma</code> in this model is higher than in the one above, suggesting a higher uncertainty in the predictions.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Plot the raw data and superimpose the MAP regression line and 89% interval for the mean and for the predicted height.</li>
</ol>
<p>We first compute the regression line by generating a sequence over the whole range of weights for which we then sample from the posterior distribution to compute a sample of mu, of which we can then compute the mean and the 89% PI.
We similarly compute the 89% PI for the predicted height.</p>
<pre class="r"><code>weight.seq &lt;- seq(from=4, to=45, length.out = 30)          
post &lt;- extract.samples(model18)          

mu &lt;- link( model18, data = list(weight = weight.seq))
mu.mean &lt;- apply(mu, 2, mean)
mu.PI &lt;- apply(mu, 2, PI, prob=0.89)

sim.height &lt;- sim( model18, data = list(weight = weight.seq ))
height.PI &lt;- apply(sim.height, 2, PI, prob=0.89)</code></pre>
<p><img src="/projects/Statistical-Rethinking/Chapter_4/chapter4_Ex_files/figure-html/unnamed-chunk-23-1.svg" width="480" /></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>What aspects of the model fit concern you?</li>
</ol>
<p>The linear model doesn’t seem to be a very good fit for the data. It performs very poorly for the lower and higher values of weight. One possibility to improve the model could be to use a polynomial model (e.g. of 2nd order) instead.</p>
<p><strong>4H3.</strong> A colleague exclaims: “Only the <em>logarithm</em> of body weight scales with height!” Let’s try this out.</p>
<ol style="list-style-type: lower-alpha">
<li>Use the entire <code>Howell1</code> data frame using the following model:
<span class="math display">\[\begin{align*}
h_i &amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp;= \alpha + \beta (\log(w_i) - \bar{x}_l) \\
\alpha &amp;\sim \text{Normal}(178, 20) \\
\beta &amp;\sim \text{Log-Normal}(0, 1) \\
\sigma &amp;\sim \text{Uniform}(0, 50)
\end{align*}\]</span>
The value <span class="math inline">\(\bar{x}_l\)</span> is the mean of the log-weights.
Here the model description in R:</li>
</ol>
<pre class="r"><code>d &lt;- Howell1
xbarl &lt;- mean( log( d$weight ) )

model.l &lt;- quap(
  alist(
    height ~ dnorm( mu, sigma) ,
    mu &lt;- a + b*( log( weight ) - xbarl ),
    a ~ dnorm( 178, 20) ,
    b ~ dlnorm( 0, 1) ,                
    sigma ~ dunif(0, 50)
  ),
  data=d
)
precis(model.l)</code></pre>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
5.5%
</th>
<th style="text-align:right;">
94.5%
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
138.27
</td>
<td style="text-align:right;">
0.220
</td>
<td style="text-align:right;">
137.92
</td>
<td style="text-align:right;">
138.62
</td>
</tr>
<tr>
<td style="text-align:left;">
b
</td>
<td style="text-align:right;">
47.07
</td>
<td style="text-align:right;">
0.383
</td>
<td style="text-align:right;">
46.46
</td>
<td style="text-align:right;">
47.68
</td>
</tr>
<tr>
<td style="text-align:left;">
sigma
</td>
<td style="text-align:right;">
5.13
</td>
<td style="text-align:right;">
0.156
</td>
<td style="text-align:right;">
4.89
</td>
<td style="text-align:right;">
5.38
</td>
</tr>
</tbody>
</table>
<p>Interpreting these results is a bit more difficult since we transformed the weights using the logarithm. The intercept <code>a</code> corresponds to the average height of someone whose log-weight is equal to the mean of log-weights, i.e. whose weight is 31kg (this is equivalent to the geometric mean).
How to interpret the <code>b</code> value?
If we increase the weight by <span class="math inline">\(p\)</span> percent (ignoring the centralization term for now), we get the following expression for <span class="math inline">\(\mu\)</span>:
<span class="math display">\[\begin{align*}
\mu &amp;= \alpha + \beta \log(\text{weight} \times (1 + p) ) 
\end{align*}\]</span>
Using some rules for logarithms, we get:
<span class="math display">\[\begin{align*}
\mu &amp;= \alpha + \beta \log(\text{weight}) + \beta \log(1 + p)
\end{align*}\]</span>
That is, an increase of <span class="math inline">\(p\)</span> percent in the weight variable is associated with an increase of <span class="math inline">\(\mu\)</span> of <span class="math inline">\(\beta \log(1 + p)\)</span>.
I personally find this not super intuitive, so let’s have a look at some plots as well. We compute again the mean <span class="math inline">\(\mu\)</span> and its compatibility interval as well as simulate predictions for the height.</p>
<pre class="r"><code>weight.seq &lt;- seq(from=2, to=70, length.out = 100)            
post &lt;- extract.samples(model.l)                           
# compute mu
mu &lt;- link( model.l, data = list(weight = weight.seq))
mu.mean &lt;- apply(mu, 2, mean)  # MAP line
mu.PI &lt;- apply(mu, 2, PI, prob=0.89)

# compute predicted height
sim.height &lt;- sim( model.l, data = list(weight = weight.seq))
height.PI &lt;- apply(sim.height, 2, PI, prob=0.89)</code></pre>
<p><img src="/projects/Statistical-Rethinking/Chapter_4/chapter4_Ex_files/figure-html/unnamed-chunk-26-1.svg" width="480" /></p>
<p>Compared to the model above fit to only the children and also compared to the models earlier in the chapter using the full data set with polynomial regression, this model seems to perform quite well on the data.</p>
<p>We can also visualize the model on log scale:</p>
<p><img src="/projects/Statistical-Rethinking/Chapter_4/chapter4_Ex_files/figure-html/unnamed-chunk-27-1.svg" width="480" /></p>
<p>Given the last two plots, I’d say the colleague was right: The logarithm of body weight scales very well with height.</p>
<p><strong>4H4.</strong> [WIP]</p>
<p><strong>4H5.</strong></p>
<p><strong>4H6.</strong></p>
<p><strong>4H7.</strong></p>
<p><small><a href="https://github.com/corriebar/Statistical-Rethinking/blob/master/Chapter_4/chapter4_Ex.Rmd">Full code.</a><small></p>
</div>
