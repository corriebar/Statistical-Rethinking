<script src="chapter5b_files/header-attrs-2.3/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#hidden-influence-in-milk">Hidden Influence in Milk</a></li>
<li><a href="#the-causal-reasoning-behind-it">The Causal Reasoning behind it</a>
<ul>
<li><a href="#markov-equivalence">Markov Equivalence</a></li>
</ul></li>
<li><a href="#simulating-a-masking-ball">Simulating a Masking Ball</a></li>
</ul>
</div>

<p>These are code snippets and notes for the fifth chapter, <em>The Many Variables &amp; The Spurious Waffles</em>, section 2, of the book “Statistical Rethinking” (version 2) by Richard McElreath.</p>
<p>I found the fifth chapter quite dense, so these notes are a bit more detailed.</p>
<div id="hidden-influence-in-milk" class="section level2">
<h2>Hidden Influence in Milk</h2>
<p>In the section about [spurious associations]({{&lt; relref “../Chapter_5/chapter5a.html” &gt;}}) we used multiple regression to eliminate variables that seemed to have an influence when comparing bivariate relationships but whose association vanishes when introducing more variables to the regression. Now the opposite can also happen: there might be no bivariate association between variables because two variables mask each other. We will explore this using the <code>milk</code> data set to compare the caloric content of milk in primates with their brain size. We will see later that the variable of the body mass also plays a role.</p>
<pre class="r"><code>library(rethinking)
data(milk)
d &lt;- milk
str(d)</code></pre>
<p>Let’s first standardize the necessary variables:</p>
<pre class="r"><code>d$K &lt;- standardize( d$kcal.per.g )
d$N &lt;- standardize( d$neocortex.perc )
d$M &lt;- standardize( log(d$mass ) )</code></pre>
<p>We first try a bivariate regression:
<span class="math display">\[\begin{align*} 
K_i &amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp;= \alpha + \beta_N N_i
\end{align*}\]</span></p>
<p>We first run this as a <code>quap()</code> model with vague priors. However, there are some missing values in the data and we need to take care of them first (otherwise the model will throw an error)</p>
<pre class="r"><code>dcc &lt;- d[ complete.cases(d$K, d$N, d$M), ]</code></pre>
<p>So now the model with the new data frame without the missing values:</p>
<pre class="r"><code>m5.5_draft &lt;- quap(
  alist(
    K ~ dnorm( mu, sigma ),
    mu &lt;- a + bN*N,
    a ~ dnorm(0, 1),
    bN ~ dnorm(0, 1),
    sigma ~ dexp( 1 )
  ), data=dcc
)</code></pre>
<p>Let’s do a quick check first how reasonable these priors are. While these models are still quite simple so these probably won’t hurt but once we do more complex models, good priors become more important.</p>
<pre class="r"><code>prior &lt;- extract.prior( m5.5_draft )
xseq &lt;- c(-2, 2)
mu &lt;- link( m5.5_draft, post=prior, data=list(N=xseq))
plot(NULL, xlim=xseq, ylim=xseq)
for (i in 1:50) lines( xseq, mu[i,], col=col.alpha(&quot;black&quot;, 0.3))</code></pre>
<p>The prior on the left is rather crazy: for the average value of <span class="math inline">\(K\)</span>, we can end up with very extreme values of <span class="math inline">\(N\)</span>. It would be better to tighten <span class="math inline">\(\alpha\)</span> so that it’s closer to the average value of the outcome variable (i.e. 0). Also, the slope can be extremely steep suggesting very unrealistic associations between the two variables, better to tigthen <span class="math inline">\(\beta\)</span> as well. Better priors (more realistic priors) would be the one in the right plot, coming from this model:</p>
<pre class="r"><code>m5.5 &lt;- quap(
  alist(
    K ~ dnorm( mu, sigma),
    mu &lt;- a + bN*N,
    a ~ dnorm(0, 0.2),
    bN ~ dnorm(0, 0.5),
    sigma ~ dexp( 1 )
  ), data=dcc
)
precis(m5.5)</code></pre>
<p>The estimate for <span class="math inline">\(\beta\)</span> is not very strong and its uncertainty interval includes 0. The same thing in visual:</p>
<p>The parameter coefficient estaimtes:</p>
<pre class="r"><code>plot( precis( m5.5))</code></pre>
<p>And the model together with the posterior mean line and interval:</p>
<pre class="r"><code>xseq &lt;- seq( from=min(dcc$N) - 0.15, to=max(dcc$N) + 0.15, length.out = 30)
mu &lt;- link( m5.5, data=list(N=xseq))
mu_mean &lt;- apply(mu, 2, mean)
mu_PI &lt;- apply(mu, 2, PI)
plot( K ~ N, data=dcc,
      xlab=&quot;neocortex percent (std)&quot;, ylab=&quot;kilocal per g (std)&quot;)
lines( xseq, mu_mean, lwd=2 )
shade( mu_PI, xseq)</code></pre>
<p>The relationship is very weak and the uncertainty interval also includes lines with no slope or even slight negative slope.</p>
<p>Let’s consider a model with the (log) body mass instead:</p>
<pre class="r"><code>m5.6 &lt;- quap(
  alist(
    K ~ dnorm( mu, sigma) ,
    mu &lt;- a + bM*M,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data=dcc
)
precis(m5.6)</code></pre>
<p>And the model together with the posterior mean line and interval:</p>
<p>The association seems stronger than for the neocortex percent but it is still highly uncertain and includes many possible weaker (as well as stronger) relationships.</p>
<p>Now let’s see what happens when we use both variables in a regression:</p>
<p><span class="math display">\[\begin{align*} 
K_i &amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp;= \alpha + \beta_N N_i + \beta_K K_i \\
\alpha &amp;\sim \text{Normal}(0, 0.2) \\
\beta_N &amp;\sim \text{Normal}(0, 0.5) \\
\beta_M &amp;\sim \text{Normal}(0, 0.5) \\
\sigma &amp;\sim \text{Exponential}(1)
\end{align*}\]</span>
And the code for the model:</p>
<pre class="r"><code>m5.7 &lt;- quap(
  alist(
    K ~ dnorm( mu, sigma),
    mu &lt;- a + bN*N + bM*M,
    a ~ dnorm(0, 0.2),
    bN ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp( 1 )
  ), data=dcc
)
precis(m5.7)</code></pre>
<p>For both variables, the observed effect increased:</p>
<pre class="r"><code>plot( coeftab( m5.5, m5.6, m5.7 ), pars=c(&quot;bM&quot;, &quot;bN&quot;) )</code></pre>
</div>
<div id="the-causal-reasoning-behind-it" class="section level2">
<h2>The Causal Reasoning behind it</h2>
<p>So what does this mean?
What happened is that both variables are correlated with the outcome but one is positively correlated (neocortex percent) and one is negatively correlated (body mass). On top of that, they’re both positively correlated with each other. In this specific situation, bigger species such as apes (high body mass) have milks will less energy. But species with more neocortex (“smarter ones”) tend to have richer milk. These correlations make it hard to see what’s really happening.</p>
<pre class="r"><code>pairs( ~ K + M + N, dcc)</code></pre>
<p>While the association between <span class="math inline">\(K\)</span> and <span class="math inline">\(M\)</span> and <span class="math inline">\(K\)</span> and <span class="math inline">\(N\)</span> are not as clear in the plot, the positive correlation between <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span> becomes very clear.</p>
<p>Let’s get some DAGs going. There are at least three DAGs consistent with these data:</p>
<pre class="r"><code>dev.off()
par(mfrow=c(1,1), bty=&quot;l&quot;)</code></pre>
<ol style="list-style-type: decimal">
<li>The body mass influences the neocortex and both influence the milk energy content.</li>
<li>The neocortex influences the body mass and both influence the milk energy content.</li>
<li>An unobserved variable <span class="math inline">\(U\)</span> influences both the body mass and the neocortex which both influence the milk energy content.</li>
</ol>
<p>How do we know which one is the correct one? The (not so satisfying) answer is we can’t know, at least not from the data alone. All three graphs imply the same set of conditional independencies (there are none). To pick one, we need to make use of our scientific knowledge.</p>
<div id="markov-equivalence" class="section level3">
<h3>Markov Equivalence</h3>
<p>A set of DAGs is known as a <em>Markov Equivalence</em> set if they all have the same implied conditional independencies.
WeWe can use <code>{{daggity}}</code> to compute the <em>Markov Equivalence</em> set of a specific DAG.</p>
<pre class="r"><code>dag5.7 &lt;- dagitty( &quot;dag{
                   M -&gt; K &lt;- N
                   M -&gt; N }&quot;)
coordinates(dag5.7) &lt;- list(x=c(M=0,K=1,N=2), y=c(M=0.5, K=1,N=0.5))
MElist &lt;- equivalentDAGs(dag5.7)
str(MElist)</code></pre>
<pre class="r"><code>n_me &lt;- length(MElist)
par(mfrow=c(2,3))
for (i in 1:n_me) {
  drawdag( MElist[i] , shapes = c(U=&quot;c&quot;), cex=1.5, radius = 4.1 )
}</code></pre>
<p>Let’s make some more counterfactual plots.</p>
<pre class="r"><code>par(mfrow=c(1,2))
xseq &lt;- seq(from=min(dcc$N)-0.15, to=max(dcc$N)+0.15, length.out = 30)
mu &lt;- link( m5.7, data=data.frame(N=xseq, M=0))
mu_mean &lt;- apply(mu, 2, mean)
mu_PI &lt;- apply(mu, 2, PI)
plot(NULL, xlim=range(dcc$N), ylim=range(dcc$K),
     xlab=&quot;neocortex percent (std)&quot;, ylab=&quot;kilocal per g (std)&quot;)
lines(xseq, mu_mean, lwd=2)
shade(mu_PI, xseq)
mtext(&quot;Counterfactual holding M = 0&quot;)

xseq &lt;- seq(from=min(dcc$M)-0.15, to=max(dcc$M)+0.15, length.out = 30)
mu &lt;- link( m5.7, data=data.frame(M=xseq, N=0))
mu_mean &lt;- apply(mu, 2, mean)
mu_PI &lt;- apply(mu, 2, PI)
plot(NULL, xlim=range(dcc$M), ylim=range(dcc$K),
     xlab=&quot;log body mass (std)&quot;, ylab=&quot;kilocal per g (std)&quot;)
lines(xseq, mu_mean, lwd=2)
shade(mu_PI, xseq)
mtext(&quot;Counterfactual holding N = 0&quot;)</code></pre>
</div>
</div>
<div id="simulating-a-masking-ball" class="section level2">
<h2>Simulating a Masking Ball</h2>
<p>Sometimes it helps to better understand the DAGs and their implied associations by using some simulations.</p>
<p>Let’s simulate data for the first DAG</p>
<pre class="r"><code>dev.off()
par(mfrow=c(1,1), bty=&quot;l&quot;)</code></pre>
<pre class="r"><code>n &lt;- 100
M &lt;- rnorm( n )
N &lt;- rnorm( n, M )
K &lt;- rnorm( n, N - M)
d_sim &lt;- data.frame(K=K, N=N, M=M)</code></pre>
<p>The second DAG:</p>
<pre class="r"><code>dev.off()
par(mfrow=c(1,1), bty=&quot;l&quot;)</code></pre>
<pre class="r"><code>n &lt;- 100
N &lt;- rnorm( n )
M &lt;- rnorm( n, N )
K &lt;- rnorm( n, N - M)
d_sim2 &lt;- data.frame(K=K, N=N, M=N)</code></pre>
<p>And the third DAG:</p>
<pre class="r"><code>dev.off()
par(mfrow=c(1,1), bty=&quot;l&quot;)</code></pre>
<pre class="r"><code>n &lt;- 100
U &lt;- rnorm ( n )
N &lt;- rnorm( n, U )
M &lt;- rnorm( n, U )
K &lt;- rnorm( n, N - M)
d_sim2 &lt;- data.frame(K=K, N=N, M=N)</code></pre>
<pre class="r"><code>plot( precis(m5.6) )

lm.seq &lt;- seq(from=-2.5, to=4.7, length.out = 30)
pred.data &lt;- data.frame( log.mass=lm.seq)

mu &lt;- link(m5.6, data=pred.data, n=1e4)
mu.mean &lt;- apply(mu, 2, mean)
mu.PI &lt;- apply(mu, 2, PI)

plot(kcal.per.g ~ log.mass, data=dcc, col=rangi2)
lines(lm.seq, mu.mean)
lines( lm.seq, mu.PI[1,], lty=2)
lines( lm.seq, mu.PI[2,], lty=2)


# fit model with both predictors together
m5.7 &lt;- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma) ,
    mu &lt;- a + bn*neocortex.perc + bm*log.mass ,
    a ~ dnorm( 0, 100),
    bn ~ dnorm(0, 1), 
    bm ~ dnorm(0, 1),
    sigma ~ dunif(0, 1)
  ), data=dcc
)

precis(m5.7)
plot( precis(m5.7))
# quite large stdev for intercept a
# compare with the single bivariate models
precis(m5.6)
precis(m5.5, digits=3)
# mass now has less of an influence, while bn has a stronger influence

# counterfactual plots
# vary neocortex.perc, keep mass fixed
mean.log.mass &lt;- mean(log(dcc$mass))
np.seq &lt;- 0:100
pred.data &lt;- data.frame(
  neocortex.perc=np.seq,
  log.mass=mean.log.mass
)

mu &lt;- link( m5.7, data=pred.data, n=1e4)
mu.mean &lt;- apply(mu, 2, mean)
mu.PI &lt;- apply( mu, 2, PI )

plot(kcal.per.g ~ neocortex.perc, data=dcc, type=&quot;n&quot;)
lines( np.seq, mu.mean)
lines( np.seq, mu.PI[1,], lty=2)
lines( np.seq, mu.PI[2,], lty=2)

# vary mass, keep neocortex.perc fixed
mean.neocortex.perc &lt;- mean(dcc$neocortex.perc)
lm.seq &lt;- seq(from=-2.5, to=4.7, length.out = 30)
pred.data &lt;- data.frame(
  neocortex.perc=mean.neocortex.perc,
  log.mass=lm.seq
)

mu &lt;- link( m5.7, data=pred.data, n=1e4)
mu.mean &lt;- apply(mu, 2, mean)
mu.PI &lt;- apply(mu, 2, PI)

plot(kcal.per.g ~ log.mass, data=dcc, type=&quot;n&quot;)
lines( lm.seq, mu.mean )
lines( lm.seq, mu.PI[1,], lty=2)
lines( lm.seq, mu.PI[2,], lty=2)

# counterfactual plots show a strong association with both variables (but in opposite direction!)



# synthetic masked association
N &lt;- 100
rho &lt;- 0.7                           # correlation between x_pos and x_neg
x_pos &lt;- rnorm( N )                  # x_pos as Gaussian
x_neg &lt;- rnorm( N, rho*x_pos,        # x_neg correlated with x_pos
                sqrt(1-rho^2) )
y &lt;- rnorm( N, x_pos - x_neg)        # y equally associated with x_pos, x_eg
d &lt;- data.frame( y, x_pos, x_neg )
pairs(d)

# bivariate models
m5.8 &lt;- map(
  alist(
    y ~ dnorm( mu, sigma) ,
    mu &lt;- a + bp*x_pos ,
    a ~ dnorm( 0, 10),
    bp ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)
  ), data=d
)
plot( precis(m5.8))

m5.9 &lt;- map(
  alist(
    y ~ dnorm( mu, sigma) ,
    mu &lt;- a +  bn*x_neg,
    a ~ dnorm( 0, 10),
    bn ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)
  ), data=d
)
plot( precis( m5.9))

# multivariate model
m5.10 &lt;- map(
  alist(
    y ~ dnorm( mu, sigma) ,
    mu &lt;- a + bp*x_pos + bn*x_neg,
    a ~ dnorm( 0, 10),
    bp ~ dnorm(0, 10),
    bn ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)
  ), data=d
)

par(mfrow=c(3,1))
plot( precis(m5.10), main=&quot;Multivariate Regression&quot;)
plot( precis(m5.9), main=&quot;Bivariate Regression, x_pos&quot;)
plot( precis(m5.9), main=&quot;Bivariate Regression x_neg&quot;)</code></pre>
</div>
